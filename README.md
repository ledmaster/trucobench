# üèÜ TrucoBench: O Campo de Batalha Definitivo para a AGI

Enquanto grandes laborat√≥rios perdem tempo ensinando LLMs a jogar xadrez ou resolver equa√ß√µes diferenciais, eu descobri *o verdadeiro caminho para a SuperIntelig√™ncia Artificial*:  

**DOMINAR O TRUCO!** ü§Øüéâ

Mas s√©rio, eu queria encontrar uma tarefa em portugu√™s que exigisse:
‚úÖ Conhecimento cultural que esteja majoritariamente em portugu√™s em textos na internet (T√™m mais materiais sobre p√¥quer do que truco em ingl√™s)
‚úÖ N√£o possa ser respondido apenas com conhecimentos gerais (por exemplo, "Quem descobriu o Brasil?")
‚úÖ Exija "racioc√≠nio" estrat√©gico, planejamento. Quero saber se o modelo consegue entender as regras ao ponto de planejar maneiras de vencer o jogo

Algumas inspira√ß√µes:
- [Tweet do Karpathy](https://x.com/karpathy/status/1885740680804504010) sugerindo a superioridade de testar LLMs usando jogos
- [SnakeBench](https://snakebench.com/): LLMs jogando o jogo da cobrinha entre eles
- [Minecraft](https://x.com/hamptonism/status/1849537031568781424): voc√™ encontra v√°rios tweets de usu√°rios comparando quais modelos constr√≥em estruturas melhores no Minecraft

## Resultados

| Modelo                              | Pontua√ß√£o | Vit | Der | % Vit |
|-------------------------------------|-----------|------|--------|----------|
| claude-3.5-sonnet                   | 2.87      | 24   | 11     | 68.6     |
| qwen-plus                           | 2.75      | 19   | 11     | 63.3     |
| o3-mini                             | 2.34      | 17   | 14     | 54.8     |
| gpt-4o                              | 2.33      | 17   | 15     | 53.1     |
| claude-3.5-haiku                    | 2.24      | 17   | 18     | 48.6     |
| qwen-turbo                          | 2.22      | 14   | 16     | 46.7     |
| qwen-max                            | 2.2       | 17   | 15     | 53.1     |
| deepseek-r1                         | 2.14      | 17   | 18     | 48.6     |
| gemini-2.0-flash-lite-preview-02-05 | 2.09      | 16   | 15     | 51.6     |
| gemini-1.5-pro                      | 2.03      | 15   | 17     | 46.9     |
| gemini-2.0-flash                    | 1.88      | 15   | 16     | 48.4     |
| gpt-4o-mini                         | 1.69      | 14   | 21     | 40.0     |
| deepseek-chat                       | 1.63      | 11   | 20     | 35.5     |

Os resultados s√£o rankeados pelo modelo [Bradley-Terry](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) que √© bastante utilizado para avaliar LLMs, inclusive para fazer o p√≥s-treino.

Pedi ao DeepSeek R1 que me ajudasse a analisar os dados das partidas, e ele me ajudou a ver padr√µes interessantes:
- O Qwen Plus foi o mais agressivo. Sempre apostando logo de primeira. Isso o ajudou a ficar em segundo lugar porque ele assustava a maioria dos modelos, que corriam das apostas.
- Modelos menores sofrem para seguir padr√µes: em v√°rios momentos eles tentam apostar fora de hora ou jogar cartas que n√£o t√™m na m√£o
- O modelo mais estrat√©gico √© o Claude-3.5-Sonnet. Ele teve uma taxa de agressividade moderada mas a segunda maior taxa de desist√™ncias e uma taxa baixa de aceita√ß√£o de apostas iniciadas pelos oponentes. Isso, junto ao fato dele ser o n√∫mero 1 em termos de vit√≥rias, sugere tra√ßos de pensamento estrat√©gico nas decis√µes.
- O segundo modelo mais estrat√©gico foi o o3-mini (avaliado com reasoning-effort='medium'). Ele tamb√©m demonstra o padr√£o estrat√©gico de apostas e desist√™ncias pr√≥ximos do Claude, mas n√£o parece decidir t√£o bem quanto o Sonnet.
- O DeepSeek V3 (deepseek-chat) era extremamente passivo. Basicamente nunca apostava e desistia com muita frequ√™ncia diante de apostas (imagine o abuso do Qwen Plus contra ele).

### Respostas Interessantes dos LLMs

#### Claude Sonnet 3.5 respondendo a um truco do Qwen Plus

Analisando a situa√ß√£o:

1. Tenho uma Dama de Paus, 3 de Espadas e 4 de Ouros
2. O 5 √© manilha em todos os naipes
3. Minhas cartas n√£o s√£o muito fortes (nenhuma manilha)
4. O advers√°rio trucou logo no in√≠cio
5. O placar est√° 0x0
6. O risco de perder 3 pontos √© consider√°vel com cartas fracas

Dado que minhas cartas s√£o relativamente fracas e o advers√°rio trucou, a decis√£o mais prudente √© correr

#### Claude Sonnet 3.5 na mesma partida

Vou analisar a situa√ß√£o:
- Tenho um √Ås de Copas, 7 de Paus e Dama de Paus
- A vira √© 2 de Copas, ent√£o as manilhas s√£o os 3s
- Tenho uma boa m√£o: 7 e Q de Paus s√£o cartas altas, e o √Ås de Copas tamb√©m √© forte
- O advers√°rio pediu truco (aposta de 3 pontos)
- Estou perdendo por 1x0
- √â a segunda rodada de apostas

Com uma m√£o relativamente forte e estando perdendo, vale a pena aumentar a aposta. A l√≥gica √©: tenho boas cartas e preciso pontuar para alcan√ßar o advers√°rio. O risco de aumentar para 6 se justifica pela for√ßa da minha m√£o e pela necessidade de pontos.

## Qu√£o s√©ria √© essa benchmark?

> "Todas as benchmarks est√£o erradas, mas algumas s√£o √∫teis."
> ***Machado de Assis***

‚ö†Ô∏è Esta benchmark foi um exerc√≠cio para eu aprender mais sobre avalia√ß√µes de LLMs e sobre o comportamento deles com prompts em portugu√™s numa atividade que exige planejamento e "racioc√≠nio" (ou algo parecido). Certamente existem bugs que, por mais que eu tenha verificado e testado, ainda est√£o no c√≥digo. Ent√£o n√£o considere essa (ou qualque outra) benchmark uma medida perfeita das capacidades dos LLMs. Al√©m disso, a qualidade dos resultados pode variar dependendo do modelo e do prompt. Aqui sigo a seguinte recomenda√ß√£o:

> "Por favor, tente garantir que os bugs afetem todos os modelos de forma igual."
> ***[Ms. Casey, Lumon Industries](https://severance.wiki/half_loop_transcript)***

## Vis√£o Geral do C√≥digo

O sistema √© composto por dois m√≥dulos principais que trabalham em conjunto para simular partidas de Truco:

1. **Motor do Jogo (engine.py)**: Implementa toda a l√≥gica do Truco paulista
2. **Partidas entre LLMs (llm_play.py)**: Respons√°vel pela integra√ß√£o com modelos de linguagem para tomada de decis√µes

## Funcionamento do C√≥digo (recomendo conhecer as regras do Truco antes de ler)

### 1. **Fase de Inicializa√ß√£o**

- Cria um baralho de 40 cartas e embaralha
- Distribui 3 cartas por jogador
- Revela a **vira** e calcula as **manilhas**

Manilhas s√£o as cartas mais fortes do jogo

---

### 2. **Andamento da M√£o**

O truco √© jogado em "m√£os" e at√© 3 rodadas dentro dessas m√£os. Cada vez que a inicializa√ß√£o acima acontece consideramos uma m√£o.

Em cada rodada os LLMs: 
   - Analisam o estado do jogo e suas cartas   
   - Decidem entre apostar, aceitar uma aposta, aumentar a aposta ou desistir da m√£o
   - Decidem quais cartas jogar, caso ningu√©m tenha desistido

Caso um LLM ganhe duas rodadas, ele ganha a m√£o e os pontos correspondentes.

---

### 3. **Partida**

A partida termina quando algum LLM atinge 12 pontos ou mais.

Isso √†s vezes acontece r√°pido (pode acontecer em uma m√£o s√≥, se os LLMs ficarem aumentando a aposta), mas √†s vezes os LLMs s√£o super passivos e ningu√©m aposta, jogando v√°rias m√£os dentro da mesma partida.

## Sistema de Sele√ß√£o dos LLMs

Para cada partida sorteei um LLM para ser o jogador A e outro para ser o B, evitando que um LLM sempre fosse o primeiro ou √∫ltimo a jogar.

A ideia era ter, pelo menos, 30 partidas de cada LLM, ent√£o determinei o peso da amostra pela f√≥rmula simples do UCB. Modelos com menos partidas totais tinham mais chances de participar.

## Aprendizados

### Verifica√ß√£o com LLMs

Para ver se as partidas estavam de acordo com as regras, usei "LLMs como ju√≠zes". 

Enviava o log da partida a um LLM e pedir para ele verificar v√°rios fatores espec√≠ficos e dizer se aquele log estava de acordo ou n√£o.

Isso foi bastante √∫til para identificar problemas com a ordem de apostas e computa√ß√£o da pontua√ß√£o das partidas.

Testei o o3-mini, o1, DeepSeek R1 e Gemini Flash Thinking para isso. Cada um exigiu um detalhe diferente no prompt, eles prestaram aten√ß√£o a aspectos diferentes da checklist que eu passei. Fica a li√ß√£o de sempre otimizar o prompt para o modelo que voc√™ vai usar. Um prompt n√£o serve para todos os modelos igualmente.