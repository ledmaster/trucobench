# üèÜ TrucoBench: O Campo de Batalha Definitivo para a AGI

Enquanto grandes laborat√≥rios perdem tempo ensinando LLMs a jogar xadrez ou resolver equa√ß√µes diferenciais, eu descobri *o verdadeiro caminho para a SuperIntelig√™ncia Artificial*:  

**DOMINAR O TRUCO!** ü§Øüéâ

Mas s√©rio, eu queria encontrar uma tarefa em portugu√™s que exigisse:
- ‚úÖ Conhecimento cultural que esteja majoritariamente em portugu√™s em textos na internet (T√™m mais materiais sobre p√¥quer do que truco em ingl√™s)
- ‚úÖ N√£o possa ser respondido apenas com conhecimentos gerais (por exemplo, "Quem descobriu o Brasil?")
- ‚úÖ Exija "racioc√≠nio" estrat√©gico, planejamento. Quero saber se o modelo consegue entender as regras ao ponto de planejar maneiras de vencer o jogo

Algumas inspira√ß√µes:
- [Tweet do Karpathy](https://x.com/karpathy/status/1885740680804504010) sugerindo a superioridade de testar LLMs usando jogos
- [SnakeBench](https://snakebench.com/): LLMs jogando o jogo da cobrinha entre eles
- [Minecraft](https://x.com/hamptonism/status/1849537031568781424): voc√™ encontra v√°rios tweets de usu√°rios comparando quais modelos constr√≥em estruturas melhores no Minecraft

## Resultados

**‚≠ê T√° gostando? Deixe uma estrela!**

Os LLMs jogaram diversas partidas entre si.

Abaixo, confira a tabela com os resultados e depois explore algumas an√°lises que destacam os pontos fortes e as peculiaridades de cada modelo.


| Modelo                              | Pontua√ß√£o | Vit | Der | % Vit |
|-------------------------------------|-----------|------|--------|----------|
| claude-3.5-sonnet                   | 2.59      | 26   | 11     | 70.3     |
| qwen-plus                           | 2.45      | 23   | 12     | 65.7     |
| o3-mini                             | 2.03      | 18   | 14     | 56.2     |
| gpt-4o                              | 1.98      | 19   | 16     | 54.3     |
| qwen-max                            | 1.96      | 23   | 17     | 57.5     |
| qwen-turbo                          | 1.95      | 16   | 16     | 50.0     |
| deepseek-r1                         | 1.86      | 19   | 18     | 51.4     |
| gemini-2.0-flash-thinking-exp-01-21 | 1.77      | 15   | 16     | 48.4     |
| claude-3.5-haiku                    | 1.74      | 18   | 22     | 45.0     |
| gemini-1.5-pro                      | 1.71      | 18   | 18     | 50.0     |
| gemini-2.0-flash-lite-preview-02-05 | 1.6       | 17   | 18     | 48.6     |
| gpt-4o-mini                         | 1.33      | 17   | 24     | 41.5     |
| gemini-2.0-flash                    | 1.32      | 15   | 19     | 44.1     |
| chatgpt-4o-latest                   | 1.28      | 13   | 21     | 38.2     |
| deepseek-chat                       | 1.2       | 11   | 21     | 34.4     |

Os resultados s√£o rankeados pelo modelo [Bradley-Terry](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) que √© bastante utilizado para avaliar LLMs, inclusive para fazer o p√≥s-treino.

Pedi ao DeepSeek R1 que me ajudasse a analisar os dados das partidas, e ele me ajudou a ver padr√µes interessantes:
- O Qwen Plus foi o mais agressivo. Sempre apostando logo de primeira. Isso o ajudou a ficar em segundo lugar porque ele assustava a maioria dos modelos, que corriam das apostas.
- Modelos menores sofrem para seguir padr√µes: em v√°rios momentos eles tentam apostar fora de hora ou jogar cartas que n√£o t√™m na m√£o
- O modelo mais estrat√©gico √© o Claude-3.5-Sonnet. Ele teve uma taxa de agressividade moderada mas a segunda maior taxa de desist√™ncias e uma taxa baixa de aceita√ß√£o de apostas iniciadas pelos oponentes. Isso, junto ao fato dele ser o n√∫mero 1 em termos de vit√≥rias, sugere tra√ßos de pensamento estrat√©gico nas decis√µes.
- O segundo modelo mais estrat√©gico foi o o3-mini (avaliado com reasoning-effort='medium'). Ele tamb√©m demonstra o padr√£o estrat√©gico de apostas e desist√™ncias pr√≥ximos do Claude, mas n√£o parece decidir t√£o bem quanto o Sonnet.
- O DeepSeek V3 (deepseek-chat) era extremamente passivo. Basicamente nunca apostava e desistia com muita frequ√™ncia diante de apostas (imagine o abuso do Qwen Plus contra ele).

O custo total aproximado para rodar as partidas foi de R$ 262,43.

### Situa√ß√µes Interessantes

#### Claude Sonnet 3.5 respondendo a um truco do Qwen Plus

Analisando a situa√ß√£o:

1. Tenho uma Dama de Paus, 3 de Espadas e 4 de Ouros
2. O 5 √© manilha em todos os naipes
3. Minhas cartas n√£o s√£o muito fortes (nenhuma manilha)
4. O advers√°rio trucou logo no in√≠cio
5. O placar est√° 0x0
6. O risco de perder 3 pontos √© consider√°vel com cartas fracas

Dado que minhas cartas s√£o relativamente fracas e o advers√°rio trucou, a decis√£o mais prudente √© correr

#### Claude Sonnet 3.5 na mesma partida

Vou analisar a situa√ß√£o:
- Tenho um √Ås de Copas, 7 de Paus e Dama de Paus
- A vira √© 2 de Copas, ent√£o as manilhas s√£o os 3s
- Tenho uma boa m√£o: 7 e Q de Paus s√£o cartas altas, e o √Ås de Copas tamb√©m √© forte
- O advers√°rio pediu truco (aposta de 3 pontos)
- Estou perdendo por 1x0
- √â a segunda rodada de apostas

Com uma m√£o relativamente forte e estando perdendo, vale a pena aumentar a aposta. A l√≥gica √©: tenho boas cartas e preciso pontuar para alcan√ßar o advers√°rio. O risco de aumentar para 6 se justifica pela for√ßa da minha m√£o e pela necessidade de pontos.

#### Qwen Plus perde do R1 porque n√£o entende o jogo

Em uma partida com placar 9x10 entre o Deepseek R1 e o Qwen Plus, o Qwen levanta a aposta para "seis". O R1 raciocina e conclui que ele n√£o pode correr, j√° que o advers√°rio ganharia 3 pontos e ganharia a partida. Como tinha manilha, o R1 decide subir a aposta para "nove" para tentar fazer o advers√°rio desistir, j√° que sua manilha era do naipe mais fraco. O Qwen Plus, n√£o entendendo que se corresse perderia o jogo automaticamente, correu com a seguinte justificativa:

Justificativa: Estamos em uma situa√ß√£o de alta aposta (9 pontos) com cartas relativamente fracas (KOP e 5OU) e precisamos proteger nosso placar atual de 10 pontos, j√° que o advers√°rio est√° muito pr√≥ximo de vencer o jogo com 9 pontos. Correr √© a melhor op√ß√£o para evitar perder muitos pontos com um m√£o desvantajosa.

#### DeepSeek R1 pensa demais e se confunde

Apesar de ter as regras do jogo especificadas no prompt inicial, o DeepSeek fica repetidamente se questionando sobre esses aspectos: "That doesn't make sense," "Maybe there's a typo," "But according to the rules...".

Ele costuma se confundi ao ver duas cartas na m√£o em vez de tr√™s (mesmo tendo a informa√ß√£o que n√£o √© a primeira rodada da m√£o) e questiona a for√ßa de cada carta com rela√ß√£o ao ranking delas em outros tipos de jogos (como p√¥quer).

J√° havia visto esse padr√£o em alguns posts no X, onde n√£o apenas o R1, mas outros modelos sentem dificuldade quando v√™em algo no prompt que vai contra o que viram durante o treinamento.

Neste caso, o R1 deve ter visto bem mais texto sobre outros jogos de cartas e precisa se esfor√ßar para superar esses padr√µes. Isso pode explicar a baixa performance.

Existe um [paper que sugere que LLMs produzem mais tokens de racioc√≠nio quando erram respostas](https://arxiv.org/abs/2501.18585).

Ele foi MUITO repetitivo nesses jogos, o que o fez ser o modelo mais caro para rodar (muitos tokens de racioc√≠nio que s√£o cobrados como "output tokens").


## Qu√£o s√©ria √© essa benchmark?

> "Todas as benchmarks est√£o erradas, mas algumas s√£o √∫teis."
> 
> ***Machado de Assis***

‚ö†Ô∏è Esta benchmark foi um exerc√≠cio para eu aprender mais sobre avalia√ß√µes de LLMs e sobre o comportamento deles com prompts em portugu√™s numa atividade que exige planejamento e "racioc√≠nio" (ou algo parecido). Certamente existem bugs que, por mais que eu tenha verificado e testado, ainda est√£o no c√≥digo. Ent√£o n√£o considere essa (ou qualque outra) benchmark uma medida perfeita das capacidades dos LLMs. Al√©m disso, a qualidade dos resultados pode variar dependendo do modelo e do prompt. Aqui sigo a seguinte recomenda√ß√£o:

> "Por favor, tente garantir que os bugs afetem todos os modelos de forma igual."
> 
> ***[Ms. Casey, Lumon Industries](https://severance.wiki/half_loop_transcript)***

## Por Dentro do TrucoBench

Disponibilizei o c√≥digo para te dar uma ideia de como fazer uma benchmark, mas voc√™ n√£o conseguir√° rod√°-lo do jeito que est√° aqui.

O sistema √© dividido em duas partes essenciais:
- **Engine do Jogo:** Respons√°vel por implementar as regras e a l√≥gica do truco (arquivo `engine.py`).
- **Integra√ß√£o com LLMs:** Gerencia as partidas e a tomada de decis√µes dos modelos (arquivo `llm_play.py`).

### Fluxo da Partida

1. **Inicializa√ß√£o:**  
   - Cria√ß√£o e embaralhamento do baralho  
   - Distribui√ß√£o de 3 cartas por jogador  
   - Defini√ß√£o da **vira** e c√°lculo das **manilhas**

2. **Andamento da M√£o:**  
   - Os LLMs analisam a situa√ß√£o e escolhem entre apostar, aceitar, aumentar ou desistir.  
   - S√£o jogadas at√© tr√™s rodadas por m√£o, onde o vencedor da maioria das rodadas leva os pontos.

3. **Final da Partida:**  
   - A partida termina quando um dos LLMs atinge 12 pontos.
   
## Sistema de Sele√ß√£o dos LLMs

Para cada partida sorteei um LLM para ser o jogador A e outro para ser o B, evitando que um LLM sempre fosse o primeiro ou √∫ltimo a jogar.

A ideia era ter, pelo menos, 30 partidas de cada LLM, ent√£o determinei o peso da amostra pela f√≥rmula simples do UCB. Modelos com menos partidas totais tinham mais chances de participar.

## Aprendizados

### Verifica√ß√£o com LLMs

Para ver se as partidas estavam de acordo com as regras, usei "LLMs como ju√≠zes". 

Enviava o log da partida a um LLM e pedir para ele verificar v√°rios fatores espec√≠ficos e dizer se aquele log estava de acordo ou n√£o.

Isso foi bastante √∫til para identificar problemas com a ordem de apostas e computa√ß√£o da pontua√ß√£o das partidas.

Testei o o3-mini, o1, DeepSeek R1 e Gemini Flash Thinking para isso. Cada um exigiu um detalhe diferente no prompt, eles prestaram aten√ß√£o a aspectos diferentes da checklist que eu passei. Fica a li√ß√£o de sempre otimizar o prompt para o modelo que voc√™ vai usar. Um prompt n√£o serve para todos os modelos igualmente.

### A formata√ß√£o do prompt importa muito!

Os verificadores tiveram imensa dificuldade em entender o andamento do jogo quando eu passava o hist√≥rico de jogadas em JSON. Quando fiz o parsing para "texto livre", a performance mudou completamente. N√£o cheguei a medir um ou outro, mas parece que mesmo em modelos modernos a formata√ß√£o do prompt (JSON, Markdown, etc) faz bastante diferen√ßa!

### Ler o racioc√≠nio e respostas de modelos que n√£o raciocinam para melhorar os prompts

Em modelos que mostram o racioc√≠nio, ou em modelos que justificam suas jogadas, √© importante ler e entender se h√° erros consistentes. Por exemplo, os modelos se mostraram confusos em rela√ß√£o √†s apostas que estavam valendo (achavam que tinham que aceitar apostas quando n√£o tinha nada pendente).

Deve dar pra melhorar isso iterando sobre os prompts para deixar mais claro. Eu fiz um prompt bem b√°sico, sem muitos truques ou itera√ß√µes, para n√£o dar vantagens aos modelos que estou acostumado a usar e, portanto, tenho um estilo de prompt mais favor√°vel.

### Nunca acredite em apenas uma benchmark

Se voc√™ olhar o ranking dos modelos, ter√° a impress√£o que o Qwen Plus √© um baita modelo bom comparado a outros como o o3-mini. Na realidade, ele ganha por puro bullying!

O Sonnet 3.5 parece ser o √∫nico modelo que consegue identificar e se aproveitar disso.

Por isso, nunca confie apenas em uma benchmark, veja v√°rias benchmarks e preste aten√ß√£o a modelos que v√£o bem em v√°rias tarefas diferentes.

E, claro, crie seu pr√≥prio conjunto de dados para avaliar o modelo na tarefa espec√≠fica que voc√™ quer resolver.